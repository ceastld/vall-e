2023-05-14 17:07:38,044 INFO [trainer.py:874] Training started
2023-05-14 17:07:38,049 INFO [trainer.py:893] Device: cuda:0
2023-05-14 17:07:38,049 INFO [trainer.py:894] {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 100, 'reset_interval': 200, 'valid_interval': 10000, 'env_info': {'k2-version': '1.23.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': '8802dac96a4eabfc90e967dc8dd967f06b21985e', 'k2-git-date': 'Fri Feb 24 17:56:13 2023', 'lhotse-version': '1.14.0.dev+git.3c621b5.clean', 'torch-version': '1.13.1+cu117', 'torch-cuda-available': True, 'torch-cuda-version': '11.7', 'python-version': '3.8', 'icefall-git-branch': 'main', 'icefall-git-sha1': '9a0c810-dirty', 'icefall-git-date': 'Sun May 14 16:11:42 2023', 'icefall-path': '/dellnas/home/gaoxuan/repos/valle/icefall', 'k2-path': '/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/k2/__init__.py', 'lhotse-path': '/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/__init__.py', 'hostname': 'ubuntu-gpu6', 'IP address': '10.0.1.6'}, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 50, 'start_epoch': 1, 'start_batch': 0, 'exp_dir': PosixPath('exp/valle_dev'), 'optimizer_name': 'ScaledAdam', 'scheduler_name': 'Eden', 'base_lr': 0.05, 'warmup_steps': 200, 'seed': 42, 'inf_check': False, 'save_every_n': 200, 'keep_last_k': 20, 'average_period': 0, 'accumulate_grad_steps': 4, 'dtype': 'bfloat16', 'filter_min_duration': 0.5, 'filter_max_duration': 14.0, 'train_stage': 1, 'visualize': False, 'oom_check': True, 'model_name': 'valle', 'decoder_dim': 1024, 'nhead': 16, 'num_decoder_layers': 12, 'scale_factor': 1.0, 'norm_first': True, 'add_prenet': False, 'prefix_mode': 1, 'share_embedding': True, 'prepend_bos': False, 'num_quantizers': 8, 'scaling_xformers': False, 'manifest_dir': PosixPath('data/tokenized'), 'max_duration': 80, 'bucketing_sampler': True, 'num_buckets': 6, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 0.1, 'on_the_fly_feats': False, 'shuffle': True, 'drop_last': False, 'return_cuts': True, 'num_workers': 8, 'enable_spec_aug': False, 'spec_aug_time_warp_factor': 80, 'input_strategy': 'PrecomputedFeatures', 'dataset': 'libritts', 'text_tokens': 'data/tokenized/unique_text_tokens.k2symbols', 'sampling_rate': 24000}
2023-05-14 17:07:38,049 INFO [trainer.py:896] About to create model
2023-05-14 17:07:39,016 INFO [trainer.py:903] Number of model parameters: 367386628
2023-05-14 17:07:40,882 INFO [datamodule.py:406] About to get train cuts
2023-05-14 17:07:40,883 INFO [datamodule.py:413] About to get dev cuts
2023-05-14 17:07:40,884 INFO [datamodule.py:276] Disable SpecAugment
2023-05-14 17:07:40,884 INFO [datamodule.py:278] About to create train dataset
2023-05-14 17:07:40,884 INFO [datamodule.py:307] Using DynamicBucketingSampler
2023-05-14 17:07:43,522 INFO [datamodule.py:325] About to create train dataloader
2023-05-14 17:07:43,523 INFO [datamodule.py:348] About to create dev dataset
2023-05-14 17:07:43,624 INFO [datamodule.py:368] About to create dev dataloader
2023-05-14 17:07:43,624 INFO [trainer.py:1108] Sanity check -- see if any of the batches in epoch 1 would cause OOM.
VALLE(
  (ar_text_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(512, 1024)
  )
  (nar_text_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(512, 1024)
  )
  (ar_audio_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(1025, 1024)
  )
  (ar_text_prenet): Identity()
  (ar_audio_prenet): Identity()
  (ar_text_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ar_audio_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ar_decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (ar_predict_layer): Linear(in_features=1024, out_features=1025, bias=False)
  (ar_accuracy_metric): MulticlassAccuracy()
  (nar_audio_embeddings): ModuleList(
    (0): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1025, 1024)
    )
    (1): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (2): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (3): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (4): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (5): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (6): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (7): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
  )
  (nar_text_prenet): Identity()
  (nar_audio_prenet): Identity()
  (nar_text_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (nar_audio_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (nar_decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): AdaptiveLayerNorm(
      (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
  (nar_predict_layers): ModuleList(
    (0): Linear(in_features=1024, out_features=1024, bias=False)
    (1): Linear(in_features=1024, out_features=1024, bias=False)
    (2): Linear(in_features=1024, out_features=1024, bias=False)
    (3): Linear(in_features=1024, out_features=1024, bias=False)
    (4): Linear(in_features=1024, out_features=1024, bias=False)
    (5): Linear(in_features=1024, out_features=1024, bias=False)
    (6): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (nar_stage_embeddings): ModuleList(
    (0): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (1): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (2): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (3): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (4): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (5): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (6): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
  )
  (nar_accuracy_metric): MulticlassAccuracy()
)
 AR parameter: ar_text_embedding.word_embeddings.weight
 AR parameter: ar_audio_embedding.word_embeddings.weight
 AR parameter: ar_text_position.alpha
 AR parameter: ar_audio_position.alpha
 AR parameter: ar_decoder.layers.0.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.0.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.0.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.0.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.0.linear1.weight
 AR parameter: ar_decoder.layers.0.linear1.bias
 AR parameter: ar_decoder.layers.0.linear2.weight
 AR parameter: ar_decoder.layers.0.linear2.bias
 AR parameter: ar_decoder.layers.0.norm1.weight
 AR parameter: ar_decoder.layers.0.norm1.bias
 AR parameter: ar_decoder.layers.0.norm2.weight
 AR parameter: ar_decoder.layers.0.norm2.bias
 AR parameter: ar_decoder.layers.1.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.1.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.1.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.1.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.1.linear1.weight
 AR parameter: ar_decoder.layers.1.linear1.bias
 AR parameter: ar_decoder.layers.1.linear2.weight
 AR parameter: ar_decoder.layers.1.linear2.bias
 AR parameter: ar_decoder.layers.1.norm1.weight
 AR parameter: ar_decoder.layers.1.norm1.bias
 AR parameter: ar_decoder.layers.1.norm2.weight
 AR parameter: ar_decoder.layers.1.norm2.bias
 AR parameter: ar_decoder.layers.2.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.2.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.2.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.2.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.2.linear1.weight
 AR parameter: ar_decoder.layers.2.linear1.bias
 AR parameter: ar_decoder.layers.2.linear2.weight
 AR parameter: ar_decoder.layers.2.linear2.bias
 AR parameter: ar_decoder.layers.2.norm1.weight
 AR parameter: ar_decoder.layers.2.norm1.bias
 AR parameter: ar_decoder.layers.2.norm2.weight
 AR parameter: ar_decoder.layers.2.norm2.bias
 AR parameter: ar_decoder.layers.3.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.3.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.3.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.3.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.3.linear1.weight
 AR parameter: ar_decoder.layers.3.linear1.bias
 AR parameter: ar_decoder.layers.3.linear2.weight
 AR parameter: ar_decoder.layers.3.linear2.bias
 AR parameter: ar_decoder.layers.3.norm1.weight
 AR parameter: ar_decoder.layers.3.norm1.bias
 AR parameter: ar_decoder.layers.3.norm2.weight
 AR parameter: ar_decoder.layers.3.norm2.bias
 AR parameter: ar_decoder.layers.4.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.4.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.4.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.4.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.4.linear1.weight
 AR parameter: ar_decoder.layers.4.linear1.bias
 AR parameter: ar_decoder.layers.4.linear2.weight
 AR parameter: ar_decoder.layers.4.linear2.bias
 AR parameter: ar_decoder.layers.4.norm1.weight
 AR parameter: ar_decoder.layers.4.norm1.bias
 AR parameter: ar_decoder.layers.4.norm2.weight
 AR parameter: ar_decoder.layers.4.norm2.bias
 AR parameter: ar_decoder.layers.5.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.5.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.5.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.5.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.5.linear1.weight
 AR parameter: ar_decoder.layers.5.linear1.bias
 AR parameter: ar_decoder.layers.5.linear2.weight
 AR parameter: ar_decoder.layers.5.linear2.bias
 AR parameter: ar_decoder.layers.5.norm1.weight
 AR parameter: ar_decoder.layers.5.norm1.bias
 AR parameter: ar_decoder.layers.5.norm2.weight
 AR parameter: ar_decoder.layers.5.norm2.bias
 AR parameter: ar_decoder.layers.6.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.6.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.6.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.6.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.6.linear1.weight
 AR parameter: ar_decoder.layers.6.linear1.bias
 AR parameter: ar_decoder.layers.6.linear2.weight
 AR parameter: ar_decoder.layers.6.linear2.bias
 AR parameter: ar_decoder.layers.6.norm1.weight
 AR parameter: ar_decoder.layers.6.norm1.bias
 AR parameter: ar_decoder.layers.6.norm2.weight
 AR parameter: ar_decoder.layers.6.norm2.bias
 AR parameter: ar_decoder.layers.7.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.7.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.7.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.7.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.7.linear1.weight
 AR parameter: ar_decoder.layers.7.linear1.bias
 AR parameter: ar_decoder.layers.7.linear2.weight
 AR parameter: ar_decoder.layers.7.linear2.bias
 AR parameter: ar_decoder.layers.7.norm1.weight
 AR parameter: ar_decoder.layers.7.norm1.bias
 AR parameter: ar_decoder.layers.7.norm2.weight
 AR parameter: ar_decoder.layers.7.norm2.bias
 AR parameter: ar_decoder.layers.8.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.8.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.8.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.8.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.8.linear1.weight
 AR parameter: ar_decoder.layers.8.linear1.bias
 AR parameter: ar_decoder.layers.8.linear2.weight
 AR parameter: ar_decoder.layers.8.linear2.bias
 AR parameter: ar_decoder.layers.8.norm1.weight
 AR parameter: ar_decoder.layers.8.norm1.bias
 AR parameter: ar_decoder.layers.8.norm2.weight
 AR parameter: ar_decoder.layers.8.norm2.bias
 AR parameter: ar_decoder.layers.9.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.9.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.9.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.9.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.9.linear1.weight
 AR parameter: ar_decoder.layers.9.linear1.bias
 AR parameter: ar_decoder.layers.9.linear2.weight
 AR parameter: ar_decoder.layers.9.linear2.bias
 AR parameter: ar_decoder.layers.9.norm1.weight
 AR parameter: ar_decoder.layers.9.norm1.bias
 AR parameter: ar_decoder.layers.9.norm2.weight
 AR parameter: ar_decoder.layers.9.norm2.bias
 AR parameter: ar_decoder.layers.10.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.10.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.10.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.10.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.10.linear1.weight
 AR parameter: ar_decoder.layers.10.linear1.bias
 AR parameter: ar_decoder.layers.10.linear2.weight
 AR parameter: ar_decoder.layers.10.linear2.bias
 AR parameter: ar_decoder.layers.10.norm1.weight
 AR parameter: ar_decoder.layers.10.norm1.bias
 AR parameter: ar_decoder.layers.10.norm2.weight
 AR parameter: ar_decoder.layers.10.norm2.bias
 AR parameter: ar_decoder.layers.11.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.11.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.11.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.11.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.11.linear1.weight
 AR parameter: ar_decoder.layers.11.linear1.bias
 AR parameter: ar_decoder.layers.11.linear2.weight
 AR parameter: ar_decoder.layers.11.linear2.bias
 AR parameter: ar_decoder.layers.11.norm1.weight
 AR parameter: ar_decoder.layers.11.norm1.bias
 AR parameter: ar_decoder.layers.11.norm2.weight
 AR parameter: ar_decoder.layers.11.norm2.bias
 AR parameter: ar_decoder.norm.weight
 AR parameter: ar_decoder.norm.bias
 AR parameter: ar_predict_layer.weight
Traceback (most recent call last):
  File "bin/trainer.py", line 1167, in <module>
    main()
  File "bin/trainer.py", line 1160, in main
    run(rank=0, world_size=1, args=args)
  File "bin/trainer.py", line 1021, in run
    scan_pessimistic_batches_for_oom(
  File "bin/trainer.py", line 1111, in scan_pessimistic_batches_for_oom
    batches, crit_values = find_pessimistic_batches(train_dl.sampler)
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/dataset/sampling/utils.py", line 77, in find_pessimistic_batches
    for batch in sampler:
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/dataset/sampling/base.py", line 265, in __next__
    batch = self._next_batch()
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/dataset/sampling/dynamic_bucketing.py", line 245, in _next_batch
    batch = next(self.cuts_iter)
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/dataset/sampling/dynamic_bucketing.py", line 402, in __iter__
    self._collect_cuts_in_buckets(batch_size)
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/dataset/sampling/dynamic_bucketing.py", line 412, in _collect_cuts_in_buckets
    cuts = next(self.cuts_iter)
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/dataset/sampling/dynamic.py", line 362, in __iter__
    for item in self.iterator:
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/utils.py", line 867, in streaming_shuffle
    for sample in data:
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/lazy.py", line 165, in values
    yield from self
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/lazy.py", line 165, in values
    yield from self
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/lazy.py", line 216, in __iter__
    yield from map(deserialize_item, self.source)
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/lazy.py", line 186, in __iter__
    for line in f:
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/gzip.py", line 305, in read1
    return self._buffer.read1(size)
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/_compression.py", line 68, in readinto
    data = self.read(len(byte_view))
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/gzip.py", line 485, in read
    buf = self._fp.read(io.DEFAULT_BUFFER_SIZE)
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/gzip.py", line 96, in read
    self.file.read(size-self._length+read)
KeyboardInterrupt
2023-05-14 17:09:25,575 INFO [trainer.py:874] Training started
2023-05-14 17:09:25,580 INFO [trainer.py:893] Device: cuda:0
2023-05-14 17:09:25,580 INFO [trainer.py:894] {'best_train_loss': inf, 'best_valid_loss': inf, 'best_train_epoch': -1, 'best_valid_epoch': -1, 'batch_idx_train': 0, 'log_interval': 100, 'reset_interval': 200, 'valid_interval': 10000, 'env_info': {'k2-version': '1.23.4', 'k2-build-type': 'Release', 'k2-with-cuda': True, 'k2-git-sha1': '8802dac96a4eabfc90e967dc8dd967f06b21985e', 'k2-git-date': 'Fri Feb 24 17:56:13 2023', 'lhotse-version': '1.14.0.dev+git.3c621b5.clean', 'torch-version': '1.13.1+cu117', 'torch-cuda-available': True, 'torch-cuda-version': '11.7', 'python-version': '3.8', 'icefall-git-branch': 'main', 'icefall-git-sha1': '9a0c810-dirty', 'icefall-git-date': 'Sun May 14 16:11:42 2023', 'icefall-path': '/dellnas/home/gaoxuan/repos/valle/icefall', 'k2-path': '/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/k2/__init__.py', 'lhotse-path': '/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/lhotse/__init__.py', 'hostname': 'ubuntu-gpu6', 'IP address': '10.0.1.6'}, 'world_size': 1, 'master_port': 12354, 'tensorboard': True, 'num_epochs': 50, 'start_epoch': 1, 'start_batch': 0, 'exp_dir': PosixPath('exp/valle_dev'), 'optimizer_name': 'ScaledAdam', 'scheduler_name': 'Eden', 'base_lr': 0.05, 'warmup_steps': 200, 'seed': 42, 'inf_check': False, 'save_every_n': 200, 'keep_last_k': 20, 'average_period': 0, 'accumulate_grad_steps': 4, 'dtype': 'bfloat16', 'filter_min_duration': 0.5, 'filter_max_duration': 14.0, 'train_stage': 1, 'visualize': False, 'oom_check': True, 'model_name': 'valle', 'decoder_dim': 1024, 'nhead': 16, 'num_decoder_layers': 12, 'scale_factor': 1.0, 'norm_first': True, 'add_prenet': False, 'prefix_mode': 1, 'share_embedding': True, 'prepend_bos': False, 'num_quantizers': 8, 'scaling_xformers': False, 'manifest_dir': PosixPath('data/tokenized'), 'max_duration': 80, 'bucketing_sampler': True, 'num_buckets': 6, 'concatenate_cuts': False, 'duration_factor': 1.0, 'gap': 0.1, 'on_the_fly_feats': False, 'shuffle': True, 'drop_last': False, 'return_cuts': True, 'num_workers': 8, 'enable_spec_aug': False, 'spec_aug_time_warp_factor': 80, 'input_strategy': 'PrecomputedFeatures', 'dataset': 'libritts', 'text_tokens': 'data/tokenized/unique_text_tokens.k2symbols', 'sampling_rate': 24000}
2023-05-14 17:09:25,580 INFO [trainer.py:896] About to create model
2023-05-14 17:09:26,543 INFO [trainer.py:903] Number of model parameters: 367386628
2023-05-14 17:09:28,176 INFO [datamodule.py:406] About to get train cuts
2023-05-14 17:09:28,178 INFO [datamodule.py:413] About to get dev cuts
2023-05-14 17:09:28,179 INFO [datamodule.py:276] Disable SpecAugment
2023-05-14 17:09:28,179 INFO [datamodule.py:278] About to create train dataset
2023-05-14 17:09:28,180 INFO [datamodule.py:307] Using DynamicBucketingSampler
2023-05-14 17:09:30,615 INFO [datamodule.py:325] About to create train dataloader
2023-05-14 17:09:30,616 INFO [datamodule.py:348] About to create dev dataset
2023-05-14 17:09:30,717 INFO [datamodule.py:368] About to create dev dataloader
2023-05-14 17:09:30,717 INFO [trainer.py:1108] Sanity check -- see if any of the batches in epoch 1 would cause OOM.
2023-05-14 17:10:50,319 INFO [trainer.py:1144] Maximum memory allocated so far is 13894MB
2023-05-14 17:10:50,757 INFO [trainer.py:1144] Maximum memory allocated so far is 14473MB
2023-05-14 17:10:51,483 INFO [trainer.py:1144] Maximum memory allocated so far is 14473MB
2023-05-14 17:10:52,219 INFO [trainer.py:1144] Maximum memory allocated so far is 14473MB
2023-05-14 17:10:53,552 INFO [trainer.py:1144] Maximum memory allocated so far is 14473MB
2023-05-14 17:10:54,822 INFO [trainer.py:1144] Maximum memory allocated so far is 14473MB
VALLE(
  (ar_text_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(512, 1024)
  )
  (nar_text_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(512, 1024)
  )
  (ar_audio_embedding): TokenEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
    (word_embeddings): Embedding(1025, 1024)
  )
  (ar_text_prenet): Identity()
  (ar_audio_prenet): Identity()
  (ar_text_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ar_audio_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (ar_decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (ar_predict_layer): Linear(in_features=1024, out_features=1025, bias=False)
  (ar_accuracy_metric): MulticlassAccuracy()
  (nar_audio_embeddings): ModuleList(
    (0): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1025, 1024)
    )
    (1): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (2): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (3): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (4): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (5): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (6): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
    (7): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1024, 1024)
    )
  )
  (nar_text_prenet): Identity()
  (nar_audio_prenet): Identity()
  (nar_text_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (nar_audio_position): SinePositionalEmbedding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (nar_decoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (6): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (7): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (8): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (9): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (10): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (11): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
        )
        (linear1): Linear(in_features=1024, out_features=4096, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (norm1): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (norm2): AdaptiveLayerNorm(
          (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): AdaptiveLayerNorm(
      (project_layer): Linear(in_features=1024, out_features=2048, bias=True)
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
  (nar_predict_layers): ModuleList(
    (0): Linear(in_features=1024, out_features=1024, bias=False)
    (1): Linear(in_features=1024, out_features=1024, bias=False)
    (2): Linear(in_features=1024, out_features=1024, bias=False)
    (3): Linear(in_features=1024, out_features=1024, bias=False)
    (4): Linear(in_features=1024, out_features=1024, bias=False)
    (5): Linear(in_features=1024, out_features=1024, bias=False)
    (6): Linear(in_features=1024, out_features=1024, bias=False)
  )
  (nar_stage_embeddings): ModuleList(
    (0): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (1): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (2): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (3): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (4): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (5): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
    (6): TokenEmbedding(
      (dropout): Dropout(p=0.0, inplace=False)
      (word_embeddings): Embedding(1, 1024)
    )
  )
  (nar_accuracy_metric): MulticlassAccuracy()
)
 AR parameter: ar_text_embedding.word_embeddings.weight
 AR parameter: ar_audio_embedding.word_embeddings.weight
 AR parameter: ar_text_position.alpha
 AR parameter: ar_audio_position.alpha
 AR parameter: ar_decoder.layers.0.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.0.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.0.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.0.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.0.linear1.weight
 AR parameter: ar_decoder.layers.0.linear1.bias
 AR parameter: ar_decoder.layers.0.linear2.weight
 AR parameter: ar_decoder.layers.0.linear2.bias
 AR parameter: ar_decoder.layers.0.norm1.weight
 AR parameter: ar_decoder.layers.0.norm1.bias
 AR parameter: ar_decoder.layers.0.norm2.weight
 AR parameter: ar_decoder.layers.0.norm2.bias
 AR parameter: ar_decoder.layers.1.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.1.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.1.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.1.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.1.linear1.weight
 AR parameter: ar_decoder.layers.1.linear1.bias
 AR parameter: ar_decoder.layers.1.linear2.weight
 AR parameter: ar_decoder.layers.1.linear2.bias
 AR parameter: ar_decoder.layers.1.norm1.weight
 AR parameter: ar_decoder.layers.1.norm1.bias
 AR parameter: ar_decoder.layers.1.norm2.weight
 AR parameter: ar_decoder.layers.1.norm2.bias
 AR parameter: ar_decoder.layers.2.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.2.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.2.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.2.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.2.linear1.weight
 AR parameter: ar_decoder.layers.2.linear1.bias
 AR parameter: ar_decoder.layers.2.linear2.weight
 AR parameter: ar_decoder.layers.2.linear2.bias
 AR parameter: ar_decoder.layers.2.norm1.weight
 AR parameter: ar_decoder.layers.2.norm1.bias
 AR parameter: ar_decoder.layers.2.norm2.weight
 AR parameter: ar_decoder.layers.2.norm2.bias
 AR parameter: ar_decoder.layers.3.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.3.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.3.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.3.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.3.linear1.weight
 AR parameter: ar_decoder.layers.3.linear1.bias
 AR parameter: ar_decoder.layers.3.linear2.weight
 AR parameter: ar_decoder.layers.3.linear2.bias
 AR parameter: ar_decoder.layers.3.norm1.weight
 AR parameter: ar_decoder.layers.3.norm1.bias
 AR parameter: ar_decoder.layers.3.norm2.weight
 AR parameter: ar_decoder.layers.3.norm2.bias
 AR parameter: ar_decoder.layers.4.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.4.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.4.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.4.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.4.linear1.weight
 AR parameter: ar_decoder.layers.4.linear1.bias
 AR parameter: ar_decoder.layers.4.linear2.weight
 AR parameter: ar_decoder.layers.4.linear2.bias
 AR parameter: ar_decoder.layers.4.norm1.weight
 AR parameter: ar_decoder.layers.4.norm1.bias
 AR parameter: ar_decoder.layers.4.norm2.weight
 AR parameter: ar_decoder.layers.4.norm2.bias
 AR parameter: ar_decoder.layers.5.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.5.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.5.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.5.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.5.linear1.weight
 AR parameter: ar_decoder.layers.5.linear1.bias
 AR parameter: ar_decoder.layers.5.linear2.weight
 AR parameter: ar_decoder.layers.5.linear2.bias
 AR parameter: ar_decoder.layers.5.norm1.weight
 AR parameter: ar_decoder.layers.5.norm1.bias
 AR parameter: ar_decoder.layers.5.norm2.weight
 AR parameter: ar_decoder.layers.5.norm2.bias
 AR parameter: ar_decoder.layers.6.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.6.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.6.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.6.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.6.linear1.weight
 AR parameter: ar_decoder.layers.6.linear1.bias
 AR parameter: ar_decoder.layers.6.linear2.weight
 AR parameter: ar_decoder.layers.6.linear2.bias
 AR parameter: ar_decoder.layers.6.norm1.weight
 AR parameter: ar_decoder.layers.6.norm1.bias
 AR parameter: ar_decoder.layers.6.norm2.weight
 AR parameter: ar_decoder.layers.6.norm2.bias
 AR parameter: ar_decoder.layers.7.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.7.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.7.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.7.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.7.linear1.weight
 AR parameter: ar_decoder.layers.7.linear1.bias
 AR parameter: ar_decoder.layers.7.linear2.weight
 AR parameter: ar_decoder.layers.7.linear2.bias
 AR parameter: ar_decoder.layers.7.norm1.weight
 AR parameter: ar_decoder.layers.7.norm1.bias
 AR parameter: ar_decoder.layers.7.norm2.weight
 AR parameter: ar_decoder.layers.7.norm2.bias
 AR parameter: ar_decoder.layers.8.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.8.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.8.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.8.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.8.linear1.weight
 AR parameter: ar_decoder.layers.8.linear1.bias
 AR parameter: ar_decoder.layers.8.linear2.weight
 AR parameter: ar_decoder.layers.8.linear2.bias
 AR parameter: ar_decoder.layers.8.norm1.weight
 AR parameter: ar_decoder.layers.8.norm1.bias
 AR parameter: ar_decoder.layers.8.norm2.weight
 AR parameter: ar_decoder.layers.8.norm2.bias
 AR parameter: ar_decoder.layers.9.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.9.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.9.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.9.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.9.linear1.weight
 AR parameter: ar_decoder.layers.9.linear1.bias
 AR parameter: ar_decoder.layers.9.linear2.weight
 AR parameter: ar_decoder.layers.9.linear2.bias
 AR parameter: ar_decoder.layers.9.norm1.weight
 AR parameter: ar_decoder.layers.9.norm1.bias
 AR parameter: ar_decoder.layers.9.norm2.weight
 AR parameter: ar_decoder.layers.9.norm2.bias
 AR parameter: ar_decoder.layers.10.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.10.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.10.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.10.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.10.linear1.weight
 AR parameter: ar_decoder.layers.10.linear1.bias
 AR parameter: ar_decoder.layers.10.linear2.weight
 AR parameter: ar_decoder.layers.10.linear2.bias
 AR parameter: ar_decoder.layers.10.norm1.weight
 AR parameter: ar_decoder.layers.10.norm1.bias
 AR parameter: ar_decoder.layers.10.norm2.weight
 AR parameter: ar_decoder.layers.10.norm2.bias
 AR parameter: ar_decoder.layers.11.self_attn.in_proj_weight
 AR parameter: ar_decoder.layers.11.self_attn.in_proj_bias
 AR parameter: ar_decoder.layers.11.self_attn.out_proj.weight
 AR parameter: ar_decoder.layers.11.self_attn.out_proj.bias
 AR parameter: ar_decoder.layers.11.linear1.weight
 AR parameter: ar_decoder.layers.11.linear1.bias
 AR parameter: ar_decoder.layers.11.linear2.weight
 AR parameter: ar_decoder.layers.11.linear2.bias
 AR parameter: ar_decoder.layers.11.norm1.weight
 AR parameter: ar_decoder.layers.11.norm1.bias
 AR parameter: ar_decoder.layers.11.norm2.weight
 AR parameter: ar_decoder.layers.11.norm2.bias
 AR parameter: ar_decoder.norm.weight
 AR parameter: ar_decoder.norm.bias
 AR parameter: ar_predict_layer.weight
2023-05-14 17:11:22,255 INFO [trainer.py:765] Epoch 1, batch 100, train_loss[loss=4.813, ArTop10Accuracy=0.3595, over 5413.00 frames. ], tot_loss[loss=5.334, ArTop10Accuracy=0.2904, over 2032.86 frames. ], batch size: 14, lr: 3.75e-02
2023-05-14 17:11:45,004 INFO [checkpoint.py:75] Saving checkpoint to exp/valle_dev/checkpoint-200.pt
Traceback (most recent call last):
  File "bin/trainer.py", line 1167, in <module>
    main()
  File "bin/trainer.py", line 1160, in main
    run(rank=0, world_size=1, args=args)
  File "bin/trainer.py", line 1047, in run
    train_one_epoch(
  File "bin/trainer.py", line 738, in train_one_epoch
    torch.distributed.barrier()
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 3144, in barrier
    default_pg = _get_default_group()
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 584, in _get_default_group
    raise RuntimeError(
RuntimeError: Default process group has not been initialized, please make sure to call init_process_group.
Traceback (most recent call last):
  File "bin/trainer.py", line 65, in <module>
    from valle.data import TtsDataModule
  File "/dellnas/home/gaoxuan/repos/valle/valle/valle/__init__.py", line 1, in <module>
    from . import data, models, modules, utils
  File "/dellnas/home/gaoxuan/repos/valle/valle/valle/models/__init__.py", line 13, in <module>
    from .transformer import Transformer
  File "/dellnas/home/gaoxuan/repos/valle/valle/valle/models/transformer.py", line 24, in <module>
    from valle.models.valle import Transpose
  File "/dellnas/home/gaoxuan/repos/valle/valle/valle/models/valle.py", line 35, in <module>
    from .visualizer import visualize
  File "/dellnas/home/gaoxuan/repos/valle/valle/valle/models/visualizer.py", line 21, in <module>
    import matplotlib.pyplot as plt
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/matplotlib/pyplot.py", line 52, in <module>
    import matplotlib.colorbar
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/matplotlib/colorbar.py", line 19, in <module>
    from matplotlib import _api, cbook, collections, cm, colors, contour, ticker
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/matplotlib/collections.py", line 928, in <module>
    class _CollectionWithSizes(Collection):
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/matplotlib/artist.py", line 150, in __init_subclass__
    cls._update_set_signature_and_docstring()
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/matplotlib/artist.py", line 171, in _update_set_signature_and_docstring
    for prop in ArtistInspector(cls).get_setters()
  File "/dellnas/home/gaoxuan/anaconda3/envs/valle3/lib/python3.8/site-packages/matplotlib/artist.py", line 1529, in get_setters
    for name in dir(self.o):
KeyboardInterrupt
